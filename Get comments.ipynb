{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import HTML, display\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import pickle\n",
    "import sys\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now processing  6  cluster\n",
      "Now processing  7  cluster\n",
      "Now processing  12  cluster\n",
      "Now processing  16  cluster\n",
      "Now processing  20  cluster\n",
      "Now processing  21  cluster\n",
      "Now processing  24  cluster\n",
      "Now processing  30  cluster\n",
      "Now processing  33  cluster\n",
      "Now processing  34  cluster\n",
      "Duration: 0:06:25.877341\n"
     ]
    }
   ],
   "source": [
    "def get_data_id(comment_sect, data_id):\n",
    "    '''\n",
    "    data_id последнего комента верхнего уровня с текущей страницы для ссылки на следующую страницу\n",
    "    '''      \n",
    "    for bla in comment_sect:\n",
    "        try:\n",
    "            data_id = bla['data-ts']\n",
    "        except KeyError:\n",
    "            pass\n",
    "        return data_id\n",
    "\n",
    "def get_text(text, comment):\n",
    "    '''\n",
    "    Достать текст из тега\n",
    "    '''\n",
    "    for sect in text:\n",
    "        comment += str(sect)\n",
    "    comment = re.sub('<br/>', '', comment)\n",
    "    return comment\n",
    "\n",
    "def get_author(comment_sect, users):\n",
    "    '''\n",
    "    собрать комментировавших и отвечавших юзеров для каждого коммента и сколько раз каждый из них писал\n",
    "    '''\n",
    "    for author in comment_sect.findAll('a', {'class':'b-comments__item-author'}):\n",
    "        if author:\n",
    "            for each in author:\n",
    "                for nick in each:\n",
    "                    users[nick] += 1\n",
    "\n",
    "def get_comment_text(comment_sect, each_comment):\n",
    "    '''\n",
    "    сбор непосредственно комментов и их дат\n",
    "    '''\n",
    "    for comment_itself in comment_sect.find('div', {'class':'b-comments__item m-first-level'}):\n",
    "        if comment_itself:\n",
    "            comment = ''\n",
    "            date = comment_itself.find('div', {'class':'b-comments__item-date'})\n",
    "            if date:\n",
    "                date = date['datetime']\n",
    "                text = comment_itself.find('div', {'class':'b-comments__item-text'})\n",
    "                if text:\n",
    "                    each_comment.append(get_text(text, comment))\n",
    "    return date\n",
    "\n",
    "def get_replies(comment_sect, each_comment, users):\n",
    "    '''\n",
    "    сбор ответов на комменты\n",
    "    '''\n",
    "    reply = ''\n",
    "    reply_date = 0\n",
    "    for reply_itself in comment_sect.find('div', {'class':'b-comments__list-answer'}):#'b-comments__list-items'\n",
    "        if reply_itself:\n",
    "            for for_reply_dates in reply_itself.findAll('div', {'class':'b-comments__item'}):\n",
    "                if for_reply_dates:\n",
    "                    reply_date = for_reply_dates['data-ts']\n",
    "            text = reply_itself.findAll('div', {'class':'b-comments__item-text'})\n",
    "            #date последнего ответа на коммент\n",
    "            if text:                \n",
    "                for sect in text:\n",
    "                    each_comment.append(get_text(sect, reply))\n",
    "                        \n",
    "            next_reply_page = comment_sect.find('div', {'class':'b-comments__list-answer-load'})\n",
    "            if next_reply_page:\n",
    "                try:\n",
    "                    next_reply_page['style']\n",
    "                except KeyError:\n",
    "                    next_replies(next_reply_page, users, reply_date, each_comment)\n",
    "\n",
    "def next_replies(next_reply_page, users, reply_date, each_comment):\n",
    "    '''\n",
    "    перейти на страницу со всеми ответами, выкачать их и логины юзеров\n",
    "    '''\n",
    "    next_reply = ''\n",
    "    parent_id = next_reply_page['data-parent_id']\n",
    "    reply_link = 'https://ria.ru/services/comments/get_list/?article_id='+article_id+'&parent_id='+parent_id+'&date='+str(reply_date)\n",
    "    req_reply = requests.get(reply_link)\n",
    "    soup_reply = BeautifulSoup(req_reply.text, 'lxml')\n",
    "    \n",
    "    for by_reply in soup_reply.findAll('div', {'class':'b-comments__item'}):\n",
    "        if by_reply:\n",
    "            reply_date = by_reply['data-ts']\n",
    "            get_author(by_reply, users)\n",
    "            text = by_reply.findAll('div', {'class':'b-comments__item-text'})\n",
    "            if text:\n",
    "                for sect in text:\n",
    "                    each_comment.append(get_text(sect, next_reply))\n",
    "            next_reply_page = by_reply.find('div', {'class':'b-comments__list-answer-load'})\n",
    "            if next_reply_page:\n",
    "                print('Noooooooooooooo')\n",
    "                next_replies(next_reply_page, users, reply_date)\n",
    "    return next_reply_page, users\n",
    "        \n",
    "def main_walker(comments_link, article_id, headline_comments_replies):\n",
    "    '''\n",
    "    Функция получает на вход ссылку, и собирает в массив кортежи из комметария, \n",
    "    ответов на него и словаря с пользователями и количеством комментов/ответов от них\n",
    "    '''\n",
    "    req = requests.get(comments_link)\n",
    "    soup = BeautifulSoup(req.text, 'lxml')\n",
    "    data_id = 0\n",
    "    for comment_sect in soup.findAll('div', {'class':'b-comments__list-tree'}):\n",
    "        data_id = get_data_id(comment_sect, data_id) #data_id последнего комента верхнего уровня с текущей страницы для ссылки на следующую страницу\n",
    "        date = ''\n",
    "        each_comment = []\n",
    "        date = get_comment_text(comment_sect, each_comment) #сбор непосредственно комментов и их дат\n",
    "        users = defaultdict(int)\n",
    "        get_author(comment_sect, users)\n",
    "        get_replies(comment_sect, each_comment, users)\n",
    "        headline_comments_replies.append((date, list(set(each_comment)), users))\n",
    "        \n",
    "    next_p = soup.find('div', {'class':'b-comments__list-answer'})\n",
    "    if next_p:\n",
    "        comments_link = 'https://ria.ru/services/comments/get_list/?article_id='+article_id+'&sort=desc&date='+data_id\n",
    "        main_walker(comments_link, article_id, headline_comments_replies)\n",
    "\n",
    "start_time = datetime.now()\n",
    "trial = ['https://ria.ru/world/20171010/1506569366.html']\n",
    "polemic_comments = []\n",
    "for cortege in to_get_comments:\n",
    "    polemic_cortege = []\n",
    "    print('Now processing cluster №\\t', cortege[0])\n",
    "    for headline in cortege[1]:\n",
    "        link = cortege[1][headline]\n",
    "        article_id = link[-15:-5] \n",
    "        comments_link = 'https://ria.ru/services/comments/get_list/?article_id='+article_id+'&sort=desc&add_class=m-main-list'\n",
    "        headline_comments_replies = []\n",
    "        main_walker(comments_link, article_id, headline_comments_replies)\n",
    "        polemic_cortege.append((headline, cortege[1][headline], headline_comments_replies))\n",
    "    polemic_comments.append((cortege[0], polemic_cortege))\n",
    "\n",
    "sys.setrecursionlimit(100000)\n",
    "with open('polemic_comments.pkl', 'wb') as fp:\n",
    "    pickle.dump(polemic_comments, fp)\n",
    "    \n",
    "end_time = datetime.now()\n",
    "print('Duration: {}'.format(end_time - start_time))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('polemic_comments.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(str(polemic_comments))\n",
    "#headline_comments_replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open ('polemic_comments.pkl', 'rb') as fp:\n",
    "    polemic_comments = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 18 74 4.111111111111111\n",
      "7 28 87 3.107142857142857\n",
      "12 13 30 2.3076923076923075\n",
      "16 7 31 4.428571428571429\n",
      "20 32 443 13.84375\n",
      "21 20 248 12.4\n",
      "24 8 47 5.875\n",
      "30 68 1521 22.36764705882353\n",
      "33 17 985 57.94117647058823\n",
      "34 16 334 20.875\n"
     ]
    }
   ],
   "source": [
    "\"достать все комментарии в каждом кластере\"\n",
    "only_comments_by_cluster = {}\n",
    "for clstr in polemic_comments:\n",
    "    clstr_comments = []\n",
    "    cntr_articles = 0\n",
    "    for article in clstr[1]:\n",
    "        cntr_articles += 1\n",
    "        comments_arr = article[2]\n",
    "        #print(len(comments_arr))\n",
    "        if len(comments_arr) != 0:\n",
    "            #print(comments_arr)\n",
    "            for inner_tuple in comments_arr:\n",
    "                #print(inner_tuple)\n",
    "                for comment_text in inner_tuple[1]:\n",
    "                    clstr_comments.append(comment_text)\n",
    "    cntr_comments = len(clstr_comments)\n",
    "    print(clstr[0], cntr_articles, cntr_comments, cntr_comments/cntr_articles)\n",
    "    only_comments_by_cluster[clstr[0]] = clstr_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1948"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"достать всех пользователей\"\n",
    "all_users = set()\n",
    "for clstr in polemic_comments:\n",
    "    #clstr_comments = []\n",
    "    #cntr_articles = 0\n",
    "    for article in clstr[1]:\n",
    "        #cntr_articles += 1\n",
    "        comments_arr = article[2]\n",
    "        #print(len(comments_arr))\n",
    "        if len(comments_arr) != 0:\n",
    "            #print(comments_arr)\n",
    "            for inner_tuple in comments_arr:\n",
    "                #print(inner_tuple)\n",
    "                for user in inner_tuple[2].keys():\n",
    "                    all_users.add(user)\n",
    "    #cntr_comments = len(clstr_comments)\n",
    "    #print(clstr[0], cntr_articles, cntr_comments, cntr_comments/cntr_articles)\n",
    "    #only_comments_by_cluster[clstr[0]] = clstr_comments\n",
    "len(all_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import nltk, math, codecs\n",
    "from gensim.models import Doc2Vec\n",
    "from nltk.cluster.kmeans import KMeansClusterer\n",
    "import re\n",
    "import pymorphy2\n",
    "from datetime import datetime\n",
    "\n",
    "fname = 'noStopLemma_PV-DBOW_wrd-vec_1it_2win_6mincount_alpha25-25_sz80.model'\n",
    "\n",
    "model = Doc2Vec.load(fname)\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "    txt = f.read().split('\\n')\n",
    "stw = set(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['бог даст - и во втором победит.',\n",
       " 'в не всякого сомнения, победа на выборах за марин ле пен, какие бы козни её не чинили...',\n",
       " 'как с трампом. 61% из специальной выборки, исключающей 100% тех, кто точно против.',\n",
       " 'макрон - президент франции! трамп - ненаш! черногория в нато! обидно, да, рос-патриоты?))))',\n",
       " 'ещё не вечер . а этим заказным опросам грош цена в базарный день .',\n",
       " 'ага, верю-верю! клинтоншу тоже, согласно опросов, поддерживало большинство, и победа была гарантирована. и где теперь те опросники, и их опросы- прогнозы?))',\n",
       " 'после выборов,если будет ставленник пиндосии макарон, то французов мне жалко. и больше они для меня не будут представлять великую францию.',\n",
       " 'сергей викторов, ну, зачем ты так жестоко с ними? они же очень сильно расстроятся, что не представляют для рос-патриота сереги великую францию)))))',\n",
       " 'до уличного светила, что называется, это дело народа франции.',\n",
       " 'опрос про клинтон вспомнился...',\n",
       " 'александр михайлов,  про париж точно знаешь? часто там бываешь, рос-патриот?))))',\n",
       " 'александр михайлов, сколько боярышника принял, рос-патриот,  чтобы в соседнем дворе \"париж\" увидеть?))))']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferring vectors\n",
      "done\n",
      "Cluster assigning done!\n",
      "Duration: 0:00:00.687492\n"
     ]
    }
   ],
   "source": [
    "clustersizes = []\n",
    "\n",
    "def distanceToCentroid():\n",
    "    for i in range(0, NUM_CLUSTERS):\n",
    "        clustersize = 0\n",
    "        for j in range(0, len(assigned_clusters)):\n",
    "            if (assigned_clusters[j] == i):\n",
    "                clustersize+=1\n",
    "        clustersizes.append(clustersize)\n",
    "        dist = 0.0\n",
    "        centr = kclusterer.means()[i]\n",
    "        for j in range(0, len(assigned_clusters)):\n",
    "            if (assigned_clusters[j] == i):\n",
    "                dist += pow(nltk.cluster.util.cosine_distance(vectors[j], centr),2)/clustersize\n",
    "        dist = math.sqrt(dist)\n",
    "        print(\"distance cluster: \"+str(i)+\" RMSE: \"+str(dist)+\" clustersize: \"+str(clustersize))\n",
    "\n",
    "def nClosestToCentroid(cluster_id, n):\n",
    "    #clustersize = len(get_titles_by_cluster(cluster_id))\n",
    "    centr = kclusterer.means()[cluster_id]\n",
    "    distances = []\n",
    "    for j in range(0, len(assigned_clusters)):\n",
    "        if (assigned_clusters[j] == cluster_id):\n",
    "            distances.append((used_lines[j], nltk.cluster.util.cosine_distance(vectors[j], centr)))\n",
    "    distances = sorted(distances, key=lambda tup: tup[1])\n",
    "    return distances[:n]\n",
    "\n",
    "\n",
    "def get_titles_by_cluster(id):\n",
    "    list = []\n",
    "    for x in range(0, len(assigned_clusters)):\n",
    "        if (assigned_clusters[x] == id):\n",
    "            list.append(used_lines[x])\n",
    "    return list\n",
    "\n",
    "def get_topics(titles):\n",
    "    from collections import Counter\n",
    "    words = [preprocess_document(x) for x in titles]\n",
    "    words = [word for sublist in words for word in sublist]\n",
    "    #filtered_words = [word for word in words if word not in stw]\n",
    "    count = Counter(words)\n",
    "    print(count.most_common()[:5])\n",
    "\n",
    "\n",
    "def cluster_to_topics(id):\n",
    "    get_topics(get_titles_by_cluster(id))\n",
    "    \n",
    "\n",
    "NUM_CLUSTERS = 5\n",
    "\n",
    "def preprocess(str):\n",
    "    # remove links\n",
    "    str = re.sub(r'http(s)?:\\/\\/\\S*? ', \"\", str)\n",
    "    return str\n",
    "\n",
    "\n",
    "def preprocess_document(text):\n",
    "    #text = preprocess(text)\n",
    "    fixedNoStop = []\n",
    "    fixed = ''.join([x if x.isalnum() or x.isspace() else \" \" for x in text ]).split()\n",
    "    for fix in fixed:\n",
    "        if fix not in stw:\n",
    "            fix = morph.parse(fix)[0].normal_form\n",
    "            fixedNoStop.append(fix)\n",
    "    return fixedNoStop\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "#data = <sparse matrix that you would normally give to scikit>.toarray()\n",
    "\n",
    "#corpus = codecs.open('test_headlines_short.txt', mode=\"r\", encoding=\"utf-8\")\n",
    "#for cluster in only_comments_by_cluster:\n",
    "#    print(len(only_comments_by_cluster[cluster]))\n",
    "#lines = ['Бог даст - и во втором победит.', 'В не всякого сомнения, победа на выборах за Марин Ле Пен, какие бы козни её не чинили...', 'Как с Трампом. 61% из специальной выборки, исключающей 100% тех, кто точно против.', 'Макрон - Президент Франции! Трамп - ненаш! Черногория в НАТО! Обидно, да, рос-патриоты?))))', 'ещё не вечер . А этим заказным опросам грош цена в базарный день .', 'Ага, верю-верю! Клинтоншу тоже, согласно опросов, поддерживало большинство, и победа была гарантирована. И где теперь те опросники, и их опросы- прогнозы?))', 'После выборов,если будет ставленник пиндосии Макарон, то французов мне жалко. И больше они для меня не будут представлять Великую Францию.', 'Сергей Викторов, Ну, зачем ты так жестоко с ними? Они же очень сильно расстроятся, что не представляют для рос-патриота сереги Великую Францию)))))', 'До уличного светила, что называется, это дело народа Франции.', 'Опрос про Клинтон вспомнился...', 'Александр Михайлов,  Про Париж точно знаешь? Часто там бываешь, рос-патриот?))))', 'Александр Михайлов, Сколько боярышника принял, рос-патриот,  чтобы в соседнем дворе \"Париж\" увидеть?))))']\n",
    "lines = only_comments_by_cluster['6']\n",
    "for line in range(len(lines)):\n",
    "    lines[line] = lines[line].lower()\n",
    "#lines = corpus.read().lower().split('\\r\\n')\n",
    "#count = len(lines)\n",
    "\n",
    "vectors = []\n",
    "\n",
    "print(\"inferring vectors\")\n",
    "duplicate_dict = {}\n",
    "used_lines = []\n",
    "for i, t in enumerate(lines):\n",
    "    if t not in duplicate_dict:#i % 2 == 0 and\n",
    "        duplicate_dict[t] = True\n",
    "        used_lines.append(t)\n",
    "        vectors.append(model.infer_vector(preprocess_document(t)))\n",
    "\n",
    "print(\"done\")\n",
    "\n",
    "\n",
    "\n",
    "kclusterer = KMeansClusterer(NUM_CLUSTERS, distance=nltk.cluster.util.cosine_distance, repeats=10)\n",
    "assigned_clusters = kclusterer.cluster(vectors, assign_clusters=True)\n",
    "print('Cluster assigning done!')\n",
    "    \n",
    "end_time = datetime.now()\n",
    "print('Duration: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('наверное, для демократии такие дебаты обязательны.и понимаешь (в какой-то мере) с кем имеешь дело, и политики узнают реальные отличия позиций друг друга.только наши, к сожалению, с прямым диалогом и открытыми вопросами не справятся. да и не нужно им это: воровать мешает.поэтому и игнорируют такие дебаты.', 0.22148576339227521), ('ага, верю-верю! клинтоншу тоже, согласно опросов, поддерживало большинство, и победа была гарантирована. и где теперь те опросники, и их опросы- прогнозы?))', 0.24988775907149763), ('dlinny_nos, тут не говорить надо, а действовать.', 0.27439817795254129), ('ну это ещё бабушка надвое сказала.трамп пример.клинтонше тоже сулили победу и все её сторонники  решили что и без его голоса она наберёт достаточно. а получилось наоборот.так что это нагнетание с макароном может сыграть шуточку.', 0.27934439057138183), ('лишь бы не получилось как с трампом, запевшим теперь другое по отношению к нам.', 0.30556859483454846)]\n",
      "####\n",
      "\n",
      "[('в не всякого сомнения, победа на выборах за марин ле пен, какие бы козни её не чинили...', 0.1579020048556472), ('live4444, вдобавок, как бы не подсчитали, как и в сша, кладбищенские голоса!', 0.19039952045581388), ('мари ле пен набрала пока в первой оценке 23% и макрон столько же. оценка была проведена французским подсчётом около восьми часов вечера в париже. точные счета будут объявлены около десяти часов вечером сегодня. завтра будут подсчитаны все голоса. во всяком случае оба идут голова к голове по среднеевропейскому времени. фион и другие уже объяснили, что проиграли выборы.', 0.21910038238997953), ('где то уже это видели. в сша тоже по всем рейтингам выигрывала клинтон!!! а в итоге? само правительство их и составляет. обьективности там ноль.', 0.23485759683882745)]\n",
      "####\n",
      "\n",
      "[('чаяния французов должны исполниться - марин ле пен будет президентом франции!!!', 0.16644152987035865), ('макрон - не макрон, просто для россии хуже уже всё равно не будет. не знаю какой он внутриполитический деятель, но внешнеполитический - очевидная марионетка (тот самый олланд)...', 0.17356805835809119), ('позволю себе высказать мнение. возможно, ле пен не идеальна, но отдать голос за макрона может только слепой, глухой и не очень умный человек. впрочем, это - дело французов, кем быть и с кем быть.', 0.18555394970115258), ('после выборов,если будет ставленник пиндосии макарон, то французов мне жалко. и больше они для меня не будут представлять великую францию.', 0.18877666160532725), ('макрона протащат, мари ле пен проиграет, у неё нет шансов', 0.19127570597749854), ('ле пен эдакий ленин в юбке. абсолютное дно, которое не поддерживает большинство в стране.', 0.20539453571965183), ('live4444, клинтон набрала больше голосов избирателей, чем трамп, который вылез только за счёт выборщикам, которых во франции нет. опросы не врали.', 0.2197235269412926), ('браво  марин!макрон-марионетка.', 0.22373611042039798)]\n",
      "####\n",
      "\n",
      "[('dlinny_nos, по клавишам нажимать, прямо как вы!', 0.29186069438956386), ('а что говорят букмекеры?', 0.2964607320611089), ('александр михайлов,  про париж точно знаешь? часто там бываешь, рос-патриот?))))', 0.29819423931359124), ('александр михайлов,  рос патриот! ты не заметил, что в москве это давно случилось?', 0.30416129283593829), ('протащат выскочку!', 0.30990211614472296)]\n",
      "####\n",
      "\n",
      "[('молодец эммануэль ! ле пен сама не знает чего хочет, никакой стратегии у неё нет, наивная женщина, делающая ставки не на то и не на тех. макрон победит в любом случае, ле пен может уже забыть о кресле президента ...  хорошо, что хоть где-то люди имеют мозги !', 0.18422885803245959), ('сергей викторов, ну, зачем ты так жестоко с ними? они же очень сильно расстроятся, что не представляют для рос-патриота сереги великую францию)))))', 0.19634471491983452), ('оланд тоже был в свое время убедительным, и что? сейчас об этом забыли? не по этим параметрам надо выбирать президента.', 0.23273301935674384)]\n",
      "####\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for clstr in range(NUM_CLUSTERS):\n",
    "    #cluster_to_topics(clstr)\n",
    "    #print(get_titles_by_cluster(clstr))\n",
    "    n = round(len(get_titles_by_cluster(clstr))/3)\n",
    "    print(nClosestToCentroid(clstr, n))\n",
    "    print('####\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
