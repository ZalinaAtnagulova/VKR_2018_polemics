{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import HTML, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get titles from one page\n",
    "def titles(section, tag, attr, title_list):\n",
    "    for i, h in enumerate(section.findAll(tag, attrs = attr)):\n",
    "        link = h.findAll('span', {'class': None})\n",
    "        if link:\n",
    "            for a in link:\n",
    "                for title in a:\n",
    "                    title_list.append(title)\n",
    "\n",
    "#Get dates from one page\n",
    "def dates(section, tag, attr, date_list):\n",
    "    for i, h in enumerate(section.findAll(tag, attrs = attr)):\n",
    "        link = h.findAll('span', {'class': None})\n",
    "        if link:\n",
    "            for a in link:\n",
    "                for art_date in a:\n",
    "                    if '.20' in art_date:\n",
    "                        date_list.append(art_date)\n",
    "\n",
    "#Extracts article's link\n",
    "def links(section, tag, link_list):\n",
    "    for i, h in enumerate(section.findAll(tag)):\n",
    "        #print(h['href'])\n",
    "        link_list.append(h['href'])\n",
    "\n",
    "#Collect titles and dates from one page\n",
    "def collector(soup, tag, attr, title_list, date_list, link_list):\n",
    "    for i, h in enumerate(soup.findAll(tag, attrs = attr)):\n",
    "        article = h.findAll('div', {'class':'b-list__item'})\n",
    "        if article:\n",
    "            for section in article:\n",
    "                titles(section, 'span', {'class':'b-list__item-title'}, title_list)\n",
    "                dates(section, 'div', {'class':'b-list__item-date'}, date_list)\n",
    "                links(section, 'a', link_list)\n",
    "\n",
    "#Find 'show more' button on the page and extract it's redirect link\n",
    "def find_button(soup, tag, attr):#'div', {'class':'b-pager'}\n",
    "    button = ''\n",
    "    for i, h in enumerate(soup.findAll(tag, attrs = attr)):\n",
    "        link = h.findAll('a')\n",
    "        if link:\n",
    "            for a in link:\n",
    "                button = a['data-ajax']\n",
    "    return button\n",
    "\n",
    "#html-parcer\n",
    "def soup_fun(section):\n",
    "    link = 'https://ria.ru/'\n",
    "    req = requests.get(link + section)\n",
    "    soup = BeautifulSoup(req.text, 'lxml')\n",
    "    return soup\n",
    "\n",
    "#walks through pages until meet the condition\n",
    "def recursor(title_list, date_list, link_list, soup):\n",
    "    print(date_list[-1])\n",
    "    try:\n",
    "        new_soup = soup_fun(find_button(soup, 'div', {'class':'b-pager'}))\n",
    "        collector(new_soup, 'div', {'class':'b-list'}, title_list, date_list, link_list)\n",
    "        if date_list[-1] != '01.01.2017' and int(date_list[-1][-4:]) >= 2017:\n",
    "            recursor(title_list, date_list, link_list, new_soup)\n",
    "    except TimeoutError:\n",
    "        raise('Oooops, TimeoutError')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.05.2018\n",
      "26.05.2018\n",
      "25.05.2018\n",
      "25.05.2018\n",
      "24.05.2018\n",
      "24.05.2018\n",
      "23.05.2018\n",
      "23.05.2018\n",
      "22.05.2018\n",
      "22.05.2018\n",
      "21.05.2018\n",
      "21.05.2018\n",
      "19.05.2018\n"
     ]
    },
   ],
   "source": [
    "req_list = ['politics', 'society', 'economy', 'incidents', 'world'] \n",
    "title_list = []\n",
    "date_list = []\n",
    "link_list = []\n",
    "\n",
    "for section in req_list:\n",
    "    soup = soup_fun(section)\n",
    "    collector(soup, 'div', {'class':'b-list'}, title_list, date_list, link_list)\n",
    "    recursor(title_list, date_list, link_list, soup)\n",
    "    print('title_list', len(title_list))\n",
    "    print('date_list', len(date_list))\n",
    "    print('link_list', len(link_list))\n",
    "    print(title_list[-1])\n",
    "\n",
    "    with open('titles_'+sect+'_till_01.01.2017.txt', 'w', encoding = 'utf-8') as f:\n",
    "    #with open('titles_world_till_01.06.2017-01.01.2017.txt', 'w', encoding = 'utf-8') as f:\n",
    "        for title in title_list:\n",
    "            f.write(title + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
