{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import cluster\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open ('polemic_comments.pkl', 'rb') as fp:\n",
    "    polemic_comments = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 \t 18 \t 74 \t 4.111111111111111\n",
      "7 \t 28 \t 87 \t 3.107142857142857\n",
      "12 \t 13 \t 30 \t 2.3076923076923075\n",
      "16 \t 7 \t 31 \t 4.428571428571429\n",
      "20 \t 32 \t 443 \t 13.84375\n",
      "21 \t 20 \t 248 \t 12.4\n",
      "24 \t 8 \t 47 \t 5.875\n",
      "30 \t 68 \t 1521 \t 22.36764705882353\n",
      "33 \t 17 \t 985 \t 57.94117647058823\n",
      "34 \t 16 \t 334 \t 20.875\n"
     ]
    }
   ],
   "source": [
    "#count cluster contents\n",
    "only_comments_by_cluster = {}\n",
    "#prints 'cluster_id', 'num_articles_in_cluster', 'num_comments_in_cluster', 'mean_comments_in_cluster'\n",
    "for clstr in polemic_comments:\n",
    "    clstr_comments = []\n",
    "    cntr_articles = 0\n",
    "    for article in clstr[1]:\n",
    "        cntr_articles += 1\n",
    "        comments_arr = article[2]\n",
    "        if len(comments_arr) != 0:\n",
    "            for inner_tuple in comments_arr:\n",
    "                for comment_text in inner_tuple[1]:\n",
    "                    clstr_comments.append(comment_text)\n",
    "    cntr_comments = len(clstr_comments)\n",
    "    print(clstr[0],'\\t',cntr_articles,'\\t',cntr_comments,'\\t',cntr_comments/cntr_articles)\n",
    "    only_comments_by_cluster[clstr[0]] = clstr_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import nltk, math, codecs\n",
    "from gensim.models import Doc2Vec\n",
    "import re\n",
    "import pymorphy2\n",
    "from datetime import datetime\n",
    "\n",
    "fname = 'noStopLemma_PV-DBOW_wrd-vec_1it_2win_6mincount_alpha25-25_sz80.model'\n",
    "\n",
    "model = Doc2Vec.load(fname)\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "    txt = f.read().split('\\n')\n",
    "stw = set(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def distanceToCentroid():\n",
    "    '''calculate distances to centroid for clusters'''\n",
    "    all_distances = []\n",
    "    for i in range(NUM_CLUSTERS):\n",
    "        clustersize = 0\n",
    "        for j in range(0, len(assigned_clusters)):\n",
    "            if (assigned_clusters[j] == i):\n",
    "                clustersize+=1\n",
    "        clustersizes.append(clustersize)\n",
    "        dist = 0.0\n",
    "        centr = km.cluster_centers_[i]\n",
    "        for j in range(0, len(assigned_clusters)):\n",
    "            if (assigned_clusters[j] == i):\n",
    "                dist += pow(nltk.cluster.util.cosine_distance(vectors[j], centr),2)/clustersize\n",
    "        dist = math.sqrt(dist)\n",
    "        all_distances.append(dist)\n",
    "    return all_distances\n",
    "\n",
    "def nClosestToCentroid(cluster_id, n):\n",
    "    '''return n headlines closest to centroid'''\n",
    "    centr = km.cluster_centers_[clstr]\n",
    "    distances = []\n",
    "    for j in range(0, len(assigned_clusters)):\n",
    "        if (assigned_clusters[j] == cluster_id):\n",
    "            distances.append((used_lines[j], nltk.cluster.util.cosine_distance(vectors[j], centr)))\n",
    "    distances = sorted(distances, key=lambda tup: tup[1])\n",
    "    return distances[:n]\n",
    "\n",
    "\n",
    "def get_titles_by_cluster(id):\n",
    "    list = []\n",
    "    for x in range(0, len(assigned_clusters)):\n",
    "        if (assigned_clusters[x] == id):\n",
    "            list.append(used_lines[x])\n",
    "    return list\n",
    "\n",
    "def get_topics(titles):\n",
    "    from collections import Counter\n",
    "    words = [preprocess_document(x) for x in titles]\n",
    "    words = [word for sublist in words for word in sublist]\n",
    "    #filtered_words = [word for word in words if word not in stw]\n",
    "    count = Counter(words)\n",
    "    print(count.most_common()[:5])\n",
    "\n",
    "\n",
    "def cluster_to_topics(id):\n",
    "    '''return 5 words best describing the topic of the cluster'''\n",
    "    get_topics(get_titles_by_cluster(id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferring vectors\n",
      "inferring vectors\n",
      "inferring vectors\n",
      "inferring vectors\n",
      "inferring vectors\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "def preprocess(str):\n",
    "    str = re.sub(r'http(s)?:\\/\\/\\S*? ', \"\", str)\n",
    "    return str\n",
    "\n",
    "\n",
    "def preprocess_document(text):\n",
    "    fixedNoStop = []\n",
    "    fixed = ''.join([x if x.isalnum() or x.isspace() else \" \" for x in text ]).split()\n",
    "    for fix in fixed:\n",
    "        if fix not in stw:\n",
    "            fix = morph.parse(fix)[0].normal_form\n",
    "            fixedNoStop.append(fix)\n",
    "    return fixedNoStop\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "comm_clusters = []\n",
    "chosen_clstrs = ['20', '21', '30', '33', '34']\n",
    "NUM_CLUSTERS = 5\n",
    "divider = 6\n",
    "    \n",
    "for polem_clstr in chosen_clstrs:\n",
    "    lines = only_comments_by_cluster[polem_clstr]\n",
    "    for line in range(len(lines)):\n",
    "        lines[line] = lines[line].lower()\n",
    "\n",
    "    vectors = []\n",
    "\n",
    "    print(\"inferring vectors\")\n",
    "    duplicate_dict = {}\n",
    "    used_lines = []\n",
    "    for i, t in enumerate(lines):\n",
    "        if t not in duplicate_dict:\n",
    "            duplicate_dict[t] = True\n",
    "            used_lines.append(t)\n",
    "            vectors.append(model.infer_vector(preprocess_document(t)))\n",
    "            \n",
    "    clustersizes = []\n",
    "    km = KMeans(n_clusters=NUM_CLUSTERS, init='k-means++', max_iter=100, n_init=8)\n",
    "    km.fit(vectors)\n",
    "    assigned_clusters = km.labels_\n",
    "    \n",
    "    theme_clstrs = []\n",
    "    for clstr in range(NUM_CLUSTERS):\n",
    "        arr = []\n",
    "        n = round(len(get_titles_by_cluster(clstr))/divider)\n",
    "        nclose = nClosestToCentroid(clstr, n)\n",
    "        for close_comm in nclose:\n",
    "            arr.append(close_comm[0])\n",
    "        theme_clstrs.append(arr)\n",
    "    comm_clusters.append((polem_clstr, theme_clstrs))\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('comments_by_themes.txt', 'w', encoding='utf-8') as f:\n",
    "    for topic in comm_clusters:\n",
    "        f.write(str(topic[0])+'\\n')\n",
    "        counter = 0\n",
    "        for comments in topic[1]:\n",
    "            f.write('Cluster_id {}\\n'.format(str(counter)))\n",
    "            for comm_cluster in comments:\n",
    "                #print(comm_cluster)\n",
    "                f.write(comm_cluster+'\\n')\n",
    "            counter = counter + 1\n",
    "            f.write('\\n')\n",
    "        f.write('\\n')\n",
    "\n",
    "#don't forget to clean output files from noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
